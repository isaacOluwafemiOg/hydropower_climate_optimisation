{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10946243,"sourceType":"datasetVersion","datasetId":6797680}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTING LIBRARIES AND READING DATA","metadata":{}},{"cell_type":"code","source":"# Importing Modules and packages\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.metrics import f1_score, mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.linear_model import LinearRegression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:22:46.037854Z","iopub.execute_input":"2025-04-14T17:22:46.038493Z","iopub.status.idle":"2025-04-14T17:22:46.043538Z","shell.execute_reply.started":"2025-04-14T17:22:46.038466Z","shell.execute_reply":"2025-04-14T17:22:46.042658Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Specifying constants and data paths\n\nWINDOWS = 14\nTRAIN_DATA_PATH = './Data/Data.csv'\nSAMPLE_SUBMISSION_PATH = 'SampleSubmission.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:22:46.044784Z","iopub.execute_input":"2025-04-14T17:22:46.045412Z","iopub.status.idle":"2025-04-14T17:22:46.065077Z","shell.execute_reply.started":"2025-04-14T17:22:46.045380Z","shell.execute_reply":"2025-04-14T17:22:46.064199Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# reading data\ntrain = pd.read_csv(TRAIN_DATA_PATH)\nss = pd.read_csv(SAMPLE_SUBMISSION_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:22:46.066487Z","iopub.execute_input":"2025-04-14T17:22:46.066820Z","iopub.status.idle":"2025-04-14T17:24:10.187374Z","shell.execute_reply.started":"2025-04-14T17:22:46.066790Z","shell.execute_reply":"2025-04-14T17:24:10.186262Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# DATA PREPROCESSING AND FEATURE ENGINEERING","metadata":{}},{"cell_type":"code","source":"# delete unwanted columns\ntrain = train.drop(['consumer_device_9','consumer_device_x'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:24:10.188459Z","iopub.execute_input":"2025-04-14T17:24:10.188725Z","iopub.status.idle":"2025-04-14T17:24:12.244680Z","shell.execute_reply.started":"2025-04-14T17:24:10.188702Z","shell.execute_reply":"2025-04-14T17:24:12.243858Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def prep_train(ndf):\n    \"\"\"\n    This function takes a DataFrame as input and performs preprocessing by:\n    - Creating a new 'device_id' column from the 'Source' column.\n    - Removing rows associated with a predefined list of unwanted device IDs.\n\n    Parameters:\n        ndf (pd.DataFrame): The input DataFrame containing at least a 'Source' column.\n\n    Returns:\n        pd.DataFrame: A cleaned DataFrame with the 'device_id' column added and \n                      rows from excluded devices removed.\n    \"\"\"\n    df = ndf.copy()\n\n    df['device_id'] = df['Source'].apply(lambda x: x.split('_data')[0])\n\n    devices_to_drop = [\"consumer_device_3\", \"consumer_device_5\", \"consumer_device_11\",\n                       \"consumer_device_14\", \"consumer_device_15\", \"consumer_device_17\",\n                       \"consumer_device_24\", \"consumer_device_25\", \"consumer_device_27\",\n                       \"consumer_device_33\", \"consumer_device_4\", \"consumer_device_9\"]\n\n    # Filter the DataFrame to exclude rows where 'device_id' is in the 'devices_to_drop' list\n    df = df[~(df['device_id'].isin(devices_to_drop))]\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:24:12.246050Z","iopub.execute_input":"2025-04-14T17:24:12.246347Z","iopub.status.idle":"2025-04-14T17:24:12.252053Z","shell.execute_reply.started":"2025-04-14T17:24:12.246303Z","shell.execute_reply":"2025-04-14T17:24:12.251334Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def datetime_features(ndf, datetime_column='date_time'):\n    \"\"\"\n    This function extracts and aggregates datetime-based features from the input DataFrame.\n    It adds year, month, and quarter features from a specified datetime column, \n    and optionally aggregates energy consumption (if the 'kwh' column is present).\n\n    Parameters:\n        ndf (pd.DataFrame): The input DataFrame containing at least the datetime column.\n        datetime_column (str): The name of the column containing datetime information.\n                               Default is 'date_time'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with new datetime features added. If 'kwh' is present,\n                      the data is aggregated daily per 'Source' and includes a binary \n                      column 'kwh_is_zero' to indicate zero consumption.\n    \"\"\"\n    # Ensure the column is in datetime format\n    df = pd.DataFrame()\n    ndf = ndf.reset_index(drop=True)\n    ndf['date'] = ndf[datetime_column].apply(lambda x: x.split(' ')[0])\n    \n    df[datetime_column] = pd.to_datetime(ndf[datetime_column])\n\n    # Date features\n    df[f'{datetime_column[:3]}_year'] = df[datetime_column].dt.year\n    df[f'{datetime_column[:3]}_month'] = df[datetime_column].dt.month\n    df[f'{datetime_column[:3]}_quarter'] = df[datetime_column].dt.quarter\n    \n    df = pd.concat([ndf.drop(datetime_column, axis=1).reset_index(drop=True),\n                    df], axis=1)\n\n    # Aggregate if energy consumption column is present\n    if 'kwh' in df.columns:        \n        df1 = df.pivot_table(index=['date', 'Source'],\n                             values=['device_id', 'dat_year', 'dat_month', 'dat_quarter'],\n                             aggfunc='first').reset_index()\n\n        df2 = df.pivot_table(index=['date', 'Source'],\n                             values=['kwh'],\n                             aggfunc='sum').reset_index()\n\n        df = pd.merge(df1, df2, on=['date', 'Source'], how='left')\n        df['kwh_is_zero'] = np.where(df['kwh'] == 0, 1, 0)\n        \n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:24:12.252989Z","iopub.execute_input":"2025-04-14T17:24:12.253277Z","iopub.status.idle":"2025-04-14T17:24:12.275031Z","shell.execute_reply.started":"2025-04-14T17:24:12.253254Z","shell.execute_reply":"2025-04-14T17:24:12.274080Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_prep = prep_train(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:24:12.276391Z","iopub.execute_input":"2025-04-14T17:24:12.276710Z","iopub.status.idle":"2025-04-14T17:24:34.110247Z","shell.execute_reply.started":"2025-04-14T17:24:12.276682Z","shell.execute_reply":"2025-04-14T17:24:34.109016Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dt = datetime_features(train_prep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:24:34.111295Z","iopub.execute_input":"2025-04-14T17:24:34.111679Z","iopub.status.idle":"2025-04-14T17:25:24.775850Z","shell.execute_reply.started":"2025-04-14T17:24:34.111652Z","shell.execute_reply":"2025-04-14T17:25:24.774579Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_ts(ndf, windows=WINDOWS):\n    \"\"\"\n    Generates lag-based features for time series modeling of energy consumption.\n\n    For each unique 'Source' in the input DataFrame, this function computes the energy consumption\n    ('kwh') from the previous `windows` days and adds them as new columns. Rows with incomplete\n    lag data are dropped.\n\n    Parameters:\n        ndf (pd.DataFrame): The input DataFrame containing at least 'Source', 'date', and 'kwh' columns.\n        windows (int, optional): The number of previous days to use for generating lag features.\n                                 Defaults to the global variable WINDOWS.\n\n    Returns:\n        pd.DataFrame: A new DataFrame where each entry includes energy consumption values\n                      from the previous `windows` days. Only rows with complete lag data are retained.\n    \"\"\"\n    df = ndf.copy()\n    new_dfs = []\n\n    for src in df['Source'].unique():\n        subset = df[df['Source'] == src].copy()\n        subset['date'] = pd.to_datetime(subset['date'])\n        for n in range(1, windows + 1):\n            subset[f'{n}-prev-date'] = subset['date'].apply(lambda x: x - relativedelta(days=n)) \n            subset = pd.merge(\n                subset,\n                subset[['date', 'kwh']],\n                left_on=f'{n}-prev-date',\n                right_on='date',\n                how='left',\n                suffixes=(\"\", f\"-prev-{n}\")\n            )\n            subset = subset.drop([f'date-prev-{n}', f'{n}-prev-date'], axis=1)\n\n        new_dfs.append(subset.dropna())\n\n    return pd.concat(new_dfs, axis=0).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:25:24.777521Z","iopub.execute_input":"2025-04-14T17:25:24.777908Z","iopub.status.idle":"2025-04-14T17:25:24.788050Z","shell.execute_reply.started":"2025-04-14T17:25:24.777876Z","shell.execute_reply":"2025-04-14T17:25:24.786972Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_ts = get_ts(train_dt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:25:24.792161Z","iopub.execute_input":"2025-04-14T17:25:24.792575Z","iopub.status.idle":"2025-04-14T17:26:13.300341Z","shell.execute_reply.started":"2025-04-14T17:25:24.792548Z","shell.execute_reply":"2025-04-14T17:26:13.299392Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def ts_fe(ndf, windows=WINDOWS):\n    \"\"\"\n    Extracts statistical and trend-based time series features from lagged energy consumption values.\n\n    This function computes a variety of features (e.g., mean, median, max, min, skewness, trend, etc.)\n    using lagged `kwh` values for each row in the input DataFrame. It assumes that lag columns \n    (e.g., 'kwh-prev-1', ..., 'kwh-prev-N') are already present.\n\n    Parameters:\n        ndf (pd.DataFrame): The input DataFrame containing lagged `kwh` columns.\n        windows (int, optional): The number of lag days to consider when extracting features.\n                                 Defaults to the global variable WINDOWS.\n\n    Returns:\n        pd.DataFrame: The original DataFrame concatenated with the newly generated feature columns.\n    \"\"\"\n    df = ndf.reset_index(drop=True).copy()\n\n    fe_dict = {}\n    ts_cols = [f'kwh-prev-{n}' for n in range(1, windows + 1)]\n    ts_df = df[ts_cols].copy()\n\n    # Statistical Features\n    fe_dict['kwh-mean'] = ts_df.mean(axis=1)\n    fe_dict['kwh-median'] = ts_df.median(axis=1)\n    fe_dict['kwh-max'] = ts_df.max(axis=1)\n    fe_dict['kwh-min'] = ts_df.min(axis=1)\n    fe_dict['kwh-skew'] = ts_df.apply(lambda row: skew(row), axis=1)\n    fe_dict['kwh-kurt'] = ts_df.apply(lambda row: kurtosis(row), axis=1)\n\n    # Trend Feature\n    def compute_trend(row):\n        X = np.arange(len(row)).reshape(-1, 1)\n        y = row.values.reshape(-1, 1)\n        model = LinearRegression().fit(X, y)\n        return model.coef_[0][0]\n\n    fe_dict['kwh-trend'] = ts_df.apply(compute_trend, axis=1)\n\n    # Differences\n    diffs = ts_df.diff(axis=1).dropna(axis=1)\n    fe_dict['kwh-diff-mean'] = diffs.mean(axis=1)\n    fe_dict['kwh-diff-min'] = diffs.min(axis=1)\n\n    # Rolling Statistics\n    fe_dict['kwh-rolling-3mean'] = ts_df.iloc[:, :3].mean(axis=1)\n    fe_dict['kwh-rolling-3std'] = ts_df.iloc[:, :3].std(axis=1)\n\n    # Relative Features\n    fe_dict['kwh-ratio-min-max'] = fe_dict['kwh-min'] / (fe_dict['kwh-max'] + 1e-6)\n    fe_dict['kwh-ratio-last-first'] = ts_df.iloc[:, -1] / (ts_df.iloc[:, 0] + 1e-6)\n\n    # Zero Count\n    fe_dict['kwh-zero_count'] = windows - (ts_df == 0).sum(axis=1)\n\n    fe_df = pd.DataFrame(fe_dict)\n\n    return pd.concat([df, fe_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:26:13.301445Z","iopub.execute_input":"2025-04-14T17:26:13.301677Z","iopub.status.idle":"2025-04-14T17:26:13.312109Z","shell.execute_reply.started":"2025-04-14T17:26:13.301661Z","shell.execute_reply":"2025-04-14T17:26:13.311157Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_fe = ts_fe(train_ts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:26:13.313026Z","iopub.execute_input":"2025-04-14T17:26:13.313356Z","iopub.status.idle":"2025-04-14T17:28:31.949361Z","shell.execute_reply.started":"2025-04-14T17:26:13.313305Z","shell.execute_reply":"2025-04-14T17:28:31.948268Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# sorting train data according date from earliest to latest\ntrain_final = train_fe.sort_values('date')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:31.950160Z","iopub.execute_input":"2025-04-14T17:28:31.950422Z","iopub.status.idle":"2025-04-14T17:28:31.983869Z","shell.execute_reply.started":"2025-04-14T17:28:31.950401Z","shell.execute_reply":"2025-04-14T17:28:31.982851Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# MODELLING","metadata":{}},{"cell_type":"markdown","source":"## Building Classifier","metadata":{}},{"cell_type":"code","source":"# specifying selected features for building classification model\nsel_ind_cl = ['dat_month',\n 'dat_quarter',\n 'kwh-prev-1',\n 'kwh-prev-2',\n 'kwh-prev-8',\n 'kwh-prev-14',\n 'kwh-median',\n 'kwh-skew',\n 'kwh-kurt',\n 'kwh-rolling-3std',\n 'kwh-ratio-min-max',\n 'kwh-zero_count']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:31.984770Z","iopub.execute_input":"2025-04-14T17:28:31.984991Z","iopub.status.idle":"2025-04-14T17:28:31.989731Z","shell.execute_reply.started":"2025-04-14T17:28:31.984975Z","shell.execute_reply":"2025-04-14T17:28:31.988823Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Partitioning data into features and target for classifier training\ntrain_X = train_final[sel_ind_cl].copy()\n\ny_cl = train_final['kwh_is_zero']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:31.990742Z","iopub.execute_input":"2025-04-14T17:28:31.991060Z","iopub.status.idle":"2025-04-14T17:28:32.023147Z","shell.execute_reply.started":"2025-04-14T17:28:31.991034Z","shell.execute_reply":"2025-04-14T17:28:32.022022Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#specifying classifier model parameters\nhyp = {'n_estimators': 898, 'max_depth': 1, 'max_leaves': 396,\n       'learning_rate': 0.000842087340956581, 'gamma': 30.94607260244057,\n       'min_child_weight': 3.8561108246221454, 'subsample': 0.861462043277969,\n       'colsample_bytree': 0.5652677799939997, 'reg_lambda': 0.7267716296342945,\n       'reg_alpha': 3.8513237586115145e-05}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:32.024389Z","iopub.execute_input":"2025-04-14T17:28:32.024671Z","iopub.status.idle":"2025-04-14T17:28:32.029603Z","shell.execute_reply.started":"2025-04-14T17:28:32.024649Z","shell.execute_reply":"2025-04-14T17:28:32.028615Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# training and evaluating classifier\nscv = StratifiedKFold(n_splits=5)\nfit_models_cl = []\nll_scores = []\ntrain_scores = []\n\nfor train_index, test_index in scv.split(train_X,y_cl):\n    X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]\n    \n    y_train, y_test = y_cl.iloc[train_index], y_cl.iloc[test_index]\n\n\n    \n    \n    model = XGBClassifier(objective='binary:logistic',random_state=42,\n                          verbosity=0,use_label_encoder=False,n_jobs=-1,\n                          **hyp)\n\n    model.fit(X_train,y_train)\n    \n    fit_models_cl.append(model)\n\n    y_pred = model.predict(X_test)\n\n\n    ind_cv = f1_score(y_test,y_pred)\n    tr_cv = f1_score(y_train,model.predict(X_train))\n    \n    ll_scores.append(ind_cv)\n    train_scores.append(tr_cv)\n\nprint('mean 5fold f1 train: ', np.mean(train_scores))\nprint('mean 5fold f1: ', np.mean(ll_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:32.030542Z","iopub.execute_input":"2025-04-14T17:28:32.030893Z","iopub.status.idle":"2025-04-14T17:28:44.586636Z","shell.execute_reply.started":"2025-04-14T17:28:32.030864Z","shell.execute_reply":"2025-04-14T17:28:44.585365Z"}},"outputs":[{"name":"stdout","text":"mean 5fold f1 train:  0.9693272290285817\nmean 5fold f1:  0.968200854168017\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Building regressor","metadata":{}},{"cell_type":"code","source":"# specifying features relevant for training regressor\nsel_ind_reg = ['dat_quarter',\n 'dat_year',\n 'kwh-prev-1',\n 'kwh-prev-2',\n 'kwh-prev-3',\n 'kwh-prev-4',\n 'kwh-prev-5',\n 'kwh-prev-6',\n 'kwh-prev-7',\n 'kwh-prev-9',\n 'kwh-prev-10',\n 'kwh-prev-11',\n 'kwh-prev-12',\n 'kwh-prev-13',\n 'kwh-prev-14',\n 'kwh-mean',\n 'kwh-median',\n 'kwh-max',\n 'kwh-min',\n 'kwh-trend',\n 'kwh-diff-mean',\n 'kwh-diff-min',\n 'kwh-rolling-3mean',\n 'kwh-rolling-3std',\n 'kwh-ratio-min-max',\n 'kwh-ratio-last-first',\n 'kwh-zero_count']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:44.587792Z","iopub.execute_input":"2025-04-14T17:28:44.588031Z","iopub.status.idle":"2025-04-14T17:28:44.593523Z","shell.execute_reply.started":"2025-04-14T17:28:44.588011Z","shell.execute_reply":"2025-04-14T17:28:44.592616Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# filtering train data to focus on non-zero consumption data for training regressor\nreg_data = train_final[train_final['kwh']!=0]\n\n#partitioning regression data into features and target\ntrain_X = reg_data[sel_ind_reg].copy()\ny = reg_data['kwh']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:44.594475Z","iopub.execute_input":"2025-04-14T17:28:44.594730Z","iopub.status.idle":"2025-04-14T17:28:44.636345Z","shell.execute_reply.started":"2025-04-14T17:28:44.594706Z","shell.execute_reply":"2025-04-14T17:28:44.635489Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#specifying best hyperparameters for training regressor model\nhyp_reg = {'n_estimators': 879, 'max_depth': 1, 'max_leaves': 315,\n           'learning_rate': 0.010081865955802438, 'gamma': 97.03393880958033,\n           'min_child_weight': 1.605808057056876, 'subsample': 0.9099095345150238,\n           'colsample_bytree': 0.8316675562349267,\n           'reg_lambda': 2.0310621003794595e-07, 'reg_alpha': 0.0020559472603671946}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:44.637651Z","iopub.execute_input":"2025-04-14T17:28:44.638522Z","iopub.status.idle":"2025-04-14T17:28:44.643089Z","shell.execute_reply.started":"2025-04-14T17:28:44.638500Z","shell.execute_reply":"2025-04-14T17:28:44.642102Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# training and evaluating regressor\nscv = KFold(n_splits=5)\nfit_models_reg = []\nll_scores = []\ntrain_scores = []\n\nfor train_index, test_index in scv.split(train_X,y):\n    X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]\n    \n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    \n    model = XGBRegressor(objective='reg:squarederror',random_state=42,\n                         verbosity=0,use_label_encoder=False,n_jobs=-1,\n                        **hyp_reg)\n\n    model.fit(X_train,y_train)\n    \n    fit_models_reg.append(model)\n\n    y_pred = model.predict(X_test)\n\n\n    ind_cv = mean_squared_error(y_test,y_pred,squared=False)\n    tr_cv = mean_squared_error(y_train,model.predict(X_train),squared=False)\n\n    \n    ll_scores.append(ind_cv)\n    train_scores.append(tr_cv)\n\nprint('mean 5fold rmse train: ', np.mean(train_scores))\nprint('mean 5fold rmse: ', np.mean(ll_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:44.644088Z","iopub.execute_input":"2025-04-14T17:28:44.644412Z","iopub.status.idle":"2025-04-14T17:28:49.009671Z","shell.execute_reply.started":"2025-04-14T17:28:44.644385Z","shell.execute_reply":"2025-04-14T17:28:49.008740Z"}},"outputs":[{"name":"stdout","text":"mean 5fold rmse train:  3.683784255326782\nmean 5fold rmse:  3.7947069607449215\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# TEST DATA","metadata":{}},{"cell_type":"markdown","source":"## Test data preparation","metadata":{}},{"cell_type":"code","source":"test = ss.copy()\ntest['date'] = test['ID'].apply(lambda x: x.split('_')[0])\ntest['date_time'] = test['ID'].apply(lambda x: x.split('_')[0])\ntest['Source'] = test['ID'].apply(lambda x: '_'.join(x.split('_')[1:]))\ntest_src = test['Source'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:49.010674Z","iopub.execute_input":"2025-04-14T17:28:49.010947Z","iopub.status.idle":"2025-04-14T17:28:49.030476Z","shell.execute_reply.started":"2025-04-14T17:28:49.010927Z","shell.execute_reply":"2025-04-14T17:28:49.029501Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"ongoing = train_prep[train_prep['Source'].isin(test_src)].copy()\nongoing['date'] = ongoing['date_time'].apply(lambda x: x.split(' ')[0]) \n\nongoing = ongoing.drop_duplicates(subset=['Source','date'])\n\ndate1 = ongoing.sort_values('date',ascending=True).drop_duplicates('Source',keep='last')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:28:49.031672Z","iopub.execute_input":"2025-04-14T17:28:49.032018Z","iopub.status.idle":"2025-04-14T17:29:06.807472Z","shell.execute_reply.started":"2025-04-14T17:28:49.031988Z","shell.execute_reply":"2025-04-14T17:29:06.806520Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Iterative test feature engineering and prediction","metadata":{}},{"cell_type":"code","source":"predictions = []\ntrain_dt_ref = train_dt[train_dt['Source'].isin(test_src)]\nfor n in range(1,32):\n    date1n = date1.copy()\n    date1n['date'] = (pd.to_datetime(date1n['date_time']) + pd.Timedelta(days=n)).astype('str')\n    date1n['date_time'] = date1n['date'].apply(lambda x: str(x) + ' 00:00:00')\n\n    \n    date1_dt = datetime_features(date1n)\n\n    date1_dt['is_test'] = 1\n    train_test = pd.concat([train_dt_ref,date1_dt],axis=0)\n    \n    traintest_ts = get_ts(train_test)\n    test_ts = traintest_ts[traintest_ts['is_test'] == 1]\n    test_fe = ts_fe(test_ts)\n\n    #pred if is zero\n    pred = {}\n    for i,model in enumerate(fit_models_cl):\n        test_prep = test_fe[sel_ind_cl] #ppipes_cl[i].transform(test_fe[sel_ind_cl])\n        ktest_sel = test_prep\n        y_prob = model.predict(ktest_sel)\n        oof = y_prob\n        pred[i] = oof    \n    pred_df = pd.DataFrame(pred)\n    pred_df = pred_df.mode(axis=1)\n    test_fe['is_zero'] = pred_df\n\n    #reg pred\n    for i,model in enumerate(fit_models_reg):\n        test_prep = test_fe[sel_ind_reg]\n        ktest_sel = test_prep\n        y_prob = model.predict(ktest_sel)\n        oof = y_prob\n        if i==0:\n            preds=oof\n        else:\n            preds = (preds + oof)\n        \n        \n    pred_df = pd.DataFrame(preds/5)\n\n    test_fe['reg_pred'] = pred_df[0]\n\n    test_fe['kwh'] = np.where(test_fe['is_zero']==1.0,0,test_fe['reg_pred'])\n\n    predictions.append(test_fe[['date','Source','kwh']])\n\n    train_dt_ref = pd.concat([train_dt_ref,\n                               test_fe[train_dt_ref.columns].reset_index(drop=True)],\n                              axis=0)\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:29:06.808573Z","iopub.execute_input":"2025-04-14T17:29:06.808836Z","iopub.status.idle":"2025-04-14T17:44:13.657398Z","shell.execute_reply.started":"2025-04-14T17:29:06.808817Z","shell.execute_reply":"2025-04-14T17:44:13.656551Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Creating Submission File","metadata":{}},{"cell_type":"code","source":"# combining predictions for separate dates in test data into one dataframe\ncompiled_pred = pd.concat(predictions,axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:13.660977Z","iopub.execute_input":"2025-04-14T17:44:13.661244Z","iopub.status.idle":"2025-04-14T17:44:13.669608Z","shell.execute_reply.started":"2025-04-14T17:44:13.661223Z","shell.execute_reply":"2025-04-14T17:44:13.668729Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Formatting predictions dataframe to match sample submission format\ncompiled_pred['ID'] = compiled_pred['date'].astype('str') + '_' + compiled_pred['Source'].astype('str')\nsub = compiled_pred[['ID','kwh']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:13.670473Z","iopub.execute_input":"2025-04-14T17:44:13.670682Z","iopub.status.idle":"2025-04-14T17:44:13.694404Z","shell.execute_reply.started":"2025-04-14T17:44:13.670666Z","shell.execute_reply":"2025-04-14T17:44:13.693545Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# exporting to submission file\nsub.to_csv('ibm_skillsbuild_9th.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:13.695303Z","iopub.execute_input":"2025-04-14T17:44:13.695579Z","iopub.status.idle":"2025-04-14T17:44:13.731182Z","shell.execute_reply.started":"2025-04-14T17:44:13.695560Z","shell.execute_reply":"2025-04-14T17:44:13.730370Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}